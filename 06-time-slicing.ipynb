{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0018d0fa-db7b-4e33-a4d4-d4402a894a43",
   "metadata": {},
   "source": [
    "# CIP203 - Maximizing GPU usage with MIGs, MPS, and Time-Slicing: \n",
    "## Time-Slicing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aac26eb-2d9d-4c57-ba1d-eb2f6552d949",
   "metadata": {},
   "source": [
    "**Questions**\n",
    "* What is Time-Slicing technology  ?\n",
    "\n",
    "**Objectives**\n",
    "* Get familiarized with the concept of NVIDIA Time-slicing\n",
    "* Learn the difference between MIG, MPS, and Time-slicing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a802f7-7880-47c1-b675-dc7568c31a0b",
   "metadata": {},
   "source": [
    "### What is Time-Slicing ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195784c5-f968-4ef3-8581-05b391d6a260",
   "metadata": {},
   "source": [
    "**Definition**\n",
    "- Time-slicing is a software-based technique that allows GPU compute resources to be shared across multiple virtual GPUs (vGPUs). With time-slicing, a physical GPU can only execute one vGPU task at a time. The GPU scheduler assigns each vGPU a slice of time in which it can execute, with other vGPUs waiting in a queue for their turn.\n",
    "- This method allows for the concurrent processing of multiple tasks by sharing the GPUâ€™s computational power in time-sequenced intervals.\n",
    "- Time-slicing is a GPU utilization technique designed to efficiently manage GPU over-subscription. It incorporates CUDA time-slicing, thereby enabling over-subscribed workloads to interleave within the GPU and take turns utilizing GPU resources.  \n",
    "\n",
    "\n",
    "**Mechanism**\n",
    "- The GPU's resources are shared by rapidly switching between different tasks, giving each one a short period to execute before moving on to the next.\n",
    "\n",
    "**Purpose**\n",
    "- It provides access to GPU resources for workloads that require intermittent access or for environments with many tasks that need to share the GPU, similar to how an operating system time-slices CPU access between processes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74336c56-5f61-4382-8c8a-094715bc01d0",
   "metadata": {},
   "source": [
    "### Characteristics of Time-Slicing:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2aac4e-009a-456a-8521-143001182b77",
   "metadata": {},
   "source": [
    "- No hardware partitioning: All jobs share the same GPU memory and compute resources without dedicated isolation.\n",
    "- Higher user density: Supports many users by quickly switching between jobs.\n",
    "- Limited isolation: Workloads can impact each other through memory contention or delayed scheduling.\n",
    "- Use Case: Suitable for bursty, low-priority tasks or general-purpose GPU access where absolute performance isolation is unnecessary.\n",
    "- Time-slicing can also extend GPU sharing to older generations that do not support MIG."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd2ec87-c797-4b12-95a3-91012ec1a60c",
   "metadata": {},
   "source": [
    "![alt text](./images/time-slicing-strict.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07050db0-13de-43e8-877c-2670e2e04e6a",
   "metadata": {},
   "source": [
    "### What's the most obvious example of time-slicing ? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5504ee27-c563-48cc-b507-63167140548d",
   "metadata": {},
   "source": [
    "It's when multiple Python threads (e.g. in PyTorch) target the same GPU. Here the GPU scheduler execute different tasks from the same CUDA context (as they originate from the same application). However, without MPS those thread calls will be serialized (aka time-sliced) without having any concurency "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51fa7aa-b53a-40ba-895f-5e4060139c82",
   "metadata": {},
   "source": [
    "### When is Time-slicing mostly applicable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030a98fe-1723-41e0-88f2-3ea006b8eda3",
   "metadata": {},
   "source": [
    "1. vGPU (Virtual GPU) environments\n",
    "2. Kubernetes clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc131412-08e1-43bc-a2b1-bd3388d6c0b2",
   "metadata": {},
   "source": [
    "## Key Points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d520e76d-8660-4f7d-a5fd-a3e5c887f1df",
   "metadata": {},
   "source": [
    "* **What is Time-Slicing**\n",
    "* **Why use Time-Slicing**\n",
    "* **Who profits from using Time-Slicing**\n",
    "* **Multiple Python threads running on the same GPU are also Time-Sliced**\n",
    "* **Time-Slicing is mostly used when multiple containers share the same GPU (Kubernetes)**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python bundle",
   "language": "python",
   "name": "ospython"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
