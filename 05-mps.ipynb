{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a222f625-0e0d-4672-a8f0-bdac2a7d15e2",
   "metadata": {},
   "source": [
    "# CIP203 - Maximizing GPU usage with MIGs, MPS, and Time-Slicing: \n",
    "## Nvidia MPS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f2b3bb-bccf-491d-829d-8af14023958b",
   "metadata": {},
   "source": [
    "Questions\n",
    "* How to speed-up my code with the use of MPS ?\n",
    "\n",
    "Objectives\n",
    "* Get familiarized with the concept of MPS\n",
    "* Learn what concurency is\n",
    "* Learn how to launch MPS daemon and change your submission script\n",
    "* Practice running tests using MPS and do benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a2b57f-7885-42ee-b31a-9a5d65f61564",
   "metadata": {},
   "source": [
    "### What is NVIDIA MPS ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29a4bdf-25dd-4581-97c5-332bca71c8df",
   "metadata": {},
   "source": [
    "NVIDIA MPS (Multi-Process Service) is a runtime service that allows multiple, individual processes to share a single NVIDIA GPU, significantly improving its utilization and performance by enabling them to run concurrently. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c43b97-94a3-4d2c-84be-217fca3feaf6",
   "metadata": {},
   "source": [
    "### Why MPS is needed ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c37c4b-33f1-47c6-89b6-6887ba93d32a",
   "metadata": {},
   "source": [
    "MPS is useful when each application process does not generate enough work to saturate the GPU. Multiple processes can be run per node using MPS to enable more concurrency. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fdee91-2d72-45fd-bacb-09355ff94265",
   "metadata": {},
   "source": [
    "### How NVIDIA MPS works"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6648e9c5-ba3c-47d3-ae7e-073c275b334e",
   "metadata": {},
   "source": [
    "1.  Shared GPU Contexts\n",
    "2.  Concurrent Kernel Execution\n",
    "3.  Reduced Overhead\n",
    "4.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff58c3d5-a4d1-4de1-8441-eb207396c378",
   "metadata": {},
   "source": [
    "The GPU has an additional \"adapter\" attached to the front-end work-delivery. Work is delivered to the GPU as if it emanated from a single process. This means that the individual tenants do not get exclusive access to the GPU; somehow the GPU is shared. Furthermore, some kind of overlap of activity may happen, which would not otherwise happen in the default case for multi-tenant/multi-process. In particular, kernel execution overlap is possible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe5beb8-6242-4ce7-b385-367d9e2e1474",
   "metadata": {},
   "source": [
    "### What is the difference between MIG and MPS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec4a2b9-12a8-4418-b4f4-8a741681aa84",
   "metadata": {},
   "source": [
    "- <font color='red'>**MIG**</font> provides full isolation, thus potentially sacrificing performance\n",
    "- <font color='blue'>**MPS**</font> focuses on maximizing performance, but does not isolate\n",
    "- <font color='red'>**MIG**</font> is suitable for sharing a GPU between different users\n",
    "- <font color='blue'>**MPS**</font> is good for running multiple tasks by the same user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903dca06-c72c-46ce-b8cd-8f36fade7444",
   "metadata": {},
   "source": [
    "### GPU farming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa72db8b-7b8d-45f7-a7d5-bbf7aad48a25",
   "metadata": {},
   "source": [
    "One of the best case scenario for using MPS:\n",
    "- when you need to run multiple instances of a CUDA application\n",
    "- BUT the application is too small to saturate a modern GPU\n",
    "\n",
    "MPS allows you to run multiple instances of the application sharing a single GPU, as long as there is enough of GPU memory for all of the instances of the application. In many cases this should result in a significantly increased throughput from all of your GPU processes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c376e02f-64ad-4f3b-a04a-4323490ff284",
   "metadata": {},
   "source": [
    "![alt text](./images/nvidia-mps.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f50efe-e1c0-4d8f-8545-cded2b066987",
   "metadata": {},
   "source": [
    "### How to enable MPS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5dcbc4-c8f4-44d5-a49b-757ecaee2777",
   "metadata": {},
   "source": [
    "MPS is not enabled by default, but it is straightforward to do. Execute the following commands in your submission script before running your CUDA application:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d318d1-f9be-4020-97d0-8627ca5857ee",
   "metadata": {},
   "source": [
    "export CUDA_MPS_PIPE_DIRECTORY=/tmp/nvidia-mps <br>\n",
    "export CUDA_MPS_LOG_DIRECTORY=/tmp/nvidia-log <br>\n",
    "nvidia-cuda-mps-control -d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f719e9c-8950-40db-9019-f1a021589741",
   "metadata": {},
   "source": [
    "- Then you can use the MPS feature if you have more than one CPU thread accessing the GPU. This will happen if you run a hybrid MPI/CUDA application, a hybrid OpenMP/CUDA application, or multiple instances of a serial CUDA application (GPU farming)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5f454f-13c3-41af-8275-38f8a49cfe58",
   "metadata": {},
   "source": [
    "- With MPS the GPU can be shared between up to 48 processes\n",
    "- There is also a memory overhead associated with using MPS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24577be-145d-4aa3-b164-ab6d981855e6",
   "metadata": {},
   "source": [
    "### Can MPS be used with MIGs ? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89ae8dd-e9c2-4e6d-b63f-fafdf218c59a",
   "metadata": {},
   "source": [
    "Absolutely yes. You can execute multiple processes on a MIG instance thus potentially increasing the GPU utilization even more. In this case though each process is expected to not being able to fully saturate even a MIG instance and use substantial amount of its memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f164afaf-101b-4114-89b4-620b096a7d58",
   "metadata": {},
   "source": [
    "### Exercise 4: Matrix multiplication using MPS (GPU kernels only)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9ed91a-ddfc-4abf-a8e7-09e3047a01e9",
   "metadata": {},
   "source": [
    "We execute the matrix multiplication code on the GPU first, trying a single process, then multiple processes and compare execution times. We also vary the matrix size (thus changing the GPU load) and see what effect it does on the concurency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23928900-df86-449e-8f36-0427fcaceb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import sys\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")\n",
    "matrix_size = 10000\n",
    "A1 = torch.randn(matrix_size, matrix_size, device=device)\n",
    "B1 = torch.randn(matrix_size, matrix_size, device=device)\n",
    "\n",
    "print(f\"Allocated GPU memory: {torch.cuda.memory_allocated() / (1024**3):.2f} GB\")\n",
    "\n",
    "# Perform matrix multiplication\n",
    "start_time = time.perf_counter()\n",
    "# Matrix multiplication on GPU\n",
    "C1 = torch.matmul(A1, B1)\n",
    "torch.cuda.synchronize()\n",
    "end_time = time.perf_counter()\n",
    "elapsed_time = end_time - start_time\n",
    "#print(\"Matrix multiplication complete.\")\n",
    "print(f\"Execution time: {elapsed_time:.6f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cff239-e88a-4474-81e2-73aa50dad623",
   "metadata": {},
   "source": [
    "### Exercise 5: Matrix multiplication using MPS (GPU kernels + CPU operations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd18c3e-97c7-4c8e-abf2-723de10ab96c",
   "metadata": {},
   "source": [
    "Now we also add matrix multiplication operation on the CPU. We first allocate matrices A_cpu and B_cpu in the CPU memory, populate them with random numbers, then execute matrix multiplication on the CPU right after the GPU execution. Then we use MPS to run multiple processes and compare the execution time. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce3508e-d69c-423b-8da7-da7c26d8e684",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e2db84-b40f-4037-831a-4c31248d5f75",
   "metadata": {},
   "source": [
    "- MPS requires to have a daemon running in the backhround\n",
    "- It is possible to run multiple processes concurently, thus increase GPU utilization\n",
    "- Concurency depends on how well processes saturate GPU\n",
    "    - If one of the processes fully use the GPU resources (be it GPU cores, memory, or registers) then execution of multiple processes will be serialized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110ee19c-05f4-4c42-948f-0812af902fed",
   "metadata": {},
   "source": [
    "## Key Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d8768b-90a7-42ae-a835-ef78aec34a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "* **What is MPS**\n",
    "* **Why use MPS**\n",
    "* **Who profits from using MPS**\n",
    "* **How to start MPS daemon**\n",
    "* **Running multiple processes using MPS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491e7be2-e313-4ea5-a5bc-0bfd6c0f8a40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python bundle",
   "language": "python",
   "name": "ospython"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
