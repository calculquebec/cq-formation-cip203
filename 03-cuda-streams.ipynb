{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85266dee-a5af-4fd7-8edc-5f38c3d28ba7",
   "metadata": {},
   "source": [
    "# CIP203 - Maximizing GPU usage with MIGs, MPS, and Time-Slicing: \n",
    "## CUDA streams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1709c9e-90c4-4504-b3d3-7b215006e9de",
   "metadata": {},
   "source": [
    "**Questions**\n",
    "* How to speed-up my code with the use of CUDA streams ?\n",
    "\n",
    "**Objectives**\n",
    "* Get familiarized with the concept of CUDA streams\n",
    "* Learn how to create cuda streams and put operations in streams in PyTorch\n",
    "* Get your feet wet by doing more exercises "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfae1a5b-87da-4a9b-964f-797a0b11f17b",
   "metadata": {},
   "source": [
    "### The problem:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8035ec-50af-4d9a-a0e8-d0b66adf5a11",
   "metadata": {},
   "source": [
    "- all the CPU threads in the same process (same CUDA context) share the same GPU\n",
    "- kernels (aka functions) are mostly serialized\n",
    "- when each fucntion is not able to fully use GPU cores, the GPU is wasted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f624ef65-5a58-4a3b-bdad-906b39946ed3",
   "metadata": {},
   "source": [
    "### CUDA Streams is the oldest NVIDIA solution for sharing a GPU between unrelated tasks:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef7c5ff-5309-46c4-a8e9-da75a45dbd55",
   "metadata": {},
   "source": [
    "- CUDA streams provide an application-layer method for sharing GPU resources by enabling <font color='red'>**CONCURRENT**</font>, asynchronous execution of tasks, such as data transfers and kernel launches. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17f2826-6f29-4712-ba91-10789dfae760",
   "metadata": {},
   "source": [
    "- Each stream is a sequence of commands executed in a specific order, but independent streams can run in parallel, potentially overlapping operations to improve performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c776cf-0711-4f9e-945f-12675018c807",
   "metadata": {},
   "source": [
    "- This allows developers to overlap memory copies with kernel execution, run multiple kernels simultaneously, and manage data dependencies more effectively to achieve higher GPU utilization. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e7e4a7-ac69-446e-8e6a-e1fde4e4414c",
   "metadata": {},
   "source": [
    "- the biggest drawback - it's limited to a single process: if your process does not have enough of parallelism to saturate modern GPU, then streams are useless"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae64fa5-2070-49dc-abc1-e2f79d9331e3",
   "metadata": {},
   "source": [
    "- another drawback: it requires re-writing the code which is time consuming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc79be1-dc65-4dc6-bf34-299b495e019b",
   "metadata": {},
   "source": [
    "### CUDA streams is like squeezing a lemon in order to get even more juice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8356c9ff-2509-47fd-9723-c643f8bae636",
   "metadata": {},
   "source": [
    "After you're done parallelizing your code and think that you did the best you could ... then think again ... you may get even more performance boost by using CUDA streams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58acf7e7-23ed-4c67-8046-7afd7415323d",
   "metadata": {},
   "source": [
    "![alt text](./images/sqeezing_lemon.jpeg \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19dd1ecd-703b-47a6-b158-5dc0e26b0908",
   "metadata": {},
   "source": [
    "### Here is how it works:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77c1643-e808-4182-9310-dd52d229ff01",
   "metadata": {},
   "source": [
    "1. Different streams are created\n",
    "2. Various operations including memory copy, kernel executions, etc are placed into different streams\n",
    "3. Streams are run <font color='red'>**CONCURRENTLY**</font>\n",
    "4. Streams are synchronized to ensure they finish at the same time\n",
    "5. Speed-up depends on availability of GPU resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d2f8b5-4010-4dfc-b2b8-7ceaf15b2952",
   "metadata": {},
   "source": [
    "### What is concurency ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e70bea-bb38-44fe-8915-887b6028c3fa",
   "metadata": {},
   "source": [
    "The ability to perform multiple CUDA operations simultaneously (beyond multi-threaded parallelism):\n",
    "- CUDA kernels (functions)\n",
    "- memory transfers from CPU to GPU\n",
    "- memory transfers from GPU to CPU\n",
    "- operations on the CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0663fc2c-13dc-43ff-a66c-9e4676348359",
   "metadata": {},
   "source": [
    "![alt text](./images/NVIDIA-CUDA-Streams.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d28d8b-7719-4ec4-9a42-48f07d7e0aec",
   "metadata": {},
   "source": [
    "### Will my code always be faster with the use of CUDA streams ? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f153ffb-5c21-412c-9cfe-b7e7ca738b70",
   "metadata": {},
   "source": [
    "It depends:\n",
    "1. on availble GPU resources (global memory, registers, shared memory, etc)\n",
    "2. on whether your code can completely saturate the GPU or not\n",
    "\n",
    "GPU resources are shared between streams. If one streams completely use all the available CUDA cores and/or memory, then other streams will have to wait, i.e. the execution will be serialized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304bfa7d-f9ad-48a0-a828-5383d7d75033",
   "metadata": {},
   "source": [
    "### Requirements for concurency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7086500b-8e19-44ad-91b0-dfbfdc5a1961",
   "metadata": {},
   "source": [
    "- GPU must support concurent operations\n",
    "- The kernels launched concurrently must not exceed the GPU's available resources, such as registers, shared memory, and compute units.\n",
    "- CUDA operations must be in different, non-0, streams\n",
    "- Kernels or memory operations in different streams should not have dependencies that force sequential executions\n",
    "- Utilize asynchronous CUDA API calls, such as cudaMemcpyAsync for memory transfers and kernel launches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e349fa8a-2398-466e-8233-109b97819961",
   "metadata": {},
   "source": [
    "### CUDA streams synchronization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe425941-d51a-4639-b919-5eec100777aa",
   "metadata": {},
   "source": [
    "**Why synchronize streams** ?\\\n",
    "CUDA streams are synchronized to ensure the correct ordering and completion of operations, particularly when dependencies exist between tasks or when the host (CPU) needs to interact with the results of GPU computations.\\\n",
    "\\\n",
    "**What if you overuse with the synchronization ?**\\\n",
    "It introduces performance bottlenecks as it forces the CPU to wait for GPU execution. Therefore, it is important to minimize unnecessary synchronization and leverage asynchronous operations and concurrent streams where possible to maximize GPU utilization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f2077c-8542-4c82-977b-9c678c473290",
   "metadata": {},
   "source": [
    "### How to invoke CUDA streams in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1181b1-5bc4-46bd-88fb-23c69aa36d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "s1 = torch.cuda.Stream()\n",
    "s2 = torch.cuda.Stream()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cb347f-3a9b-4640-b4a5-c22447fb404a",
   "metadata": {},
   "source": [
    "Here we created 2 CUDA streams in addition to the default 0 stream.<br>\n",
    "Next we need to execute certain operations in those streams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189e94f9-95c3-4e08-9928-7cf9123a48f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.cuda.stream(s1):\n",
    "    YOUR_FUNCTION\n",
    "with torch.cuda.stream(s2):\n",
    "    ANOTHER_FUNCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c26434-8087-4927-8c89-3176f39c1fa0",
   "metadata": {},
   "source": [
    "# Exercise 1: Matrix multiplications using 2 streams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbd1f71-e737-48cd-bb05-660ee397a921",
   "metadata": {},
   "source": [
    "In what follows we are going to have 2 streams. Then we distribute matrix multiplication operations between those streams. Here we also have to use NVIDIA Nsight profiler to be able to see the overlap between streams.<br>\n",
    "\n",
    "However, we will not execute the code from within the notebook. Instead, we use the Terminal to edit the exercises and the submission scripts as well as to submit sbatch jobs. <br>\n",
    "\n",
    "Please launch the Terminal and got to scripts. There you will find several Python files along with the few submission scripts. Please go ahead and open **matmul-cuda.py** first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36f00cad-0172-4e76-9cc9-d5ea1883b6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\")\n",
    "matrix_size = 10000  # Adjust this value to increase/decrease GPU intensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "216d0fc8-fbd1-44a0-9260-c17d863bfbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create  large random matrices on the chosen device\n",
    "A1 = torch.randn(matrix_size, matrix_size, device=device)\n",
    "B1 = torch.randn(matrix_size, matrix_size, device=device)\n",
    "A2 = torch.randn(matrix_size, matrix_size, device=device)\n",
    "B2 = torch.randn(matrix_size, matrix_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8670487f-bfd2-4412-9f4c-9a0499486a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Performing matrix multiplication of two {matrix_size}x{matrix_size} matrices...\")\n",
    "C1 = torch.matmul(A1, B1)\n",
    "C2 = torch.matmul(A2, B2)\n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d1d9b8-f737-45a0-bcef-4edaedd9e464",
   "metadata": {},
   "source": [
    "Use NVIDIA Nsight Systems to run&profile the code so that you can observe the concurency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a6b00c-78c7-4564-bfb7-19c9bbc678bd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<font color='blue'>**Here is how to use NVIDIA Nsight Systems profiler:**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a241ae-20d5-4bd5-86fb-9de59f21040d",
   "metadata": {},
   "source": [
    "1. NVIDIA Night System can be laucnhed from within the Launcher tab\n",
    "2. Once it's opened click \"Select target for profiling\" and chose nodegpupool.\n",
    "3. In the \"Collect CPU IP/backtrace samples\" block find \"Target application\" sub-block. Enter the following into the \"Command line with arguments\" field: python ./cq-formation-cip203-main/scripts/matmul-streams.py\n",
    "4. Click \"Start\" in the right column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b3f898-d8e4-416f-9b32-8554f6067bef",
   "metadata": {},
   "source": [
    "### Exercise 2: Create stream 3 and 4, then use them to add more matrix multiplication operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3921cd16-15b6-4c3b-ab65-66a26e6845c5",
   "metadata": {},
   "source": [
    "### What programming languages/libraries/packages have support for CUDA streams ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7402580a-bfd2-48b8-abe9-90d84aa7430b",
   "metadata": {},
   "source": [
    "- C/C++, C++ with Thrust\n",
    "- Fortran\n",
    "- Python (including packages Numba, Pytorch, PyCuda, CuPy)\n",
    "- Julia\n",
    "- Rust\n",
    "- Net\n",
    "- Java"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93adb317-d10c-41ab-b238-db952f73a4d1",
   "metadata": {},
   "source": [
    "## Key Points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79634a1b-1840-4fee-8a62-7d558aa52d0f",
   "metadata": {},
   "source": [
    "* **CUDA streams definition**\n",
    "* **Concurency**\n",
    "* **Synchronization**\n",
    "* **Are CUDA streams for everyone ?**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python bundle",
   "language": "python",
   "name": "ospython"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
